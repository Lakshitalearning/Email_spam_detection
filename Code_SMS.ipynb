{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lakshitalearning/SpamFortress/blob/main/Code_SMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geN0OYV36ds8"
      },
      "source": [
        "# **IMPORTING LIBRARIES AND DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkUUfcf6gSwy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2715dc19-253f-4ce4-8806-b2314d5bafb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "1.25.2\n",
            "2.0.3\n",
            "0.13.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#importing libraries\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "print(np.__version__)\n",
        "print(pd.__version__)\n",
        "print(sns.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh-S1jNDhmzs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "663561b3-0fa9-4189-ce80-e56063bc3f06"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'spam.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0847e6ca11a0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#reading data from csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spam.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spam.csv'"
          ]
        }
      ],
      "source": [
        "#reading data from csv file\n",
        "dataset = pd.read_csv('spam.csv',encoding='latin-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vfKrnWehz-s"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ8uxP09nhQB"
      },
      "source": [
        "# **DATA CLEANING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-lwBxU4lzo8"
      },
      "outputs": [],
      "source": [
        "#To get the knowledge of null and not null values present in data\n",
        "dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UaRg9mIn3Z6"
      },
      "outputs": [],
      "source": [
        "#since last 3 column contains many null values hence we can drop\n",
        "dataset.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5pymFoOq9r5"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehjmuk8_2KVN"
      },
      "outputs": [],
      "source": [
        "#renaming column v1 and v2\n",
        "dataset.rename(columns={'v1':'target','v2':'data'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmLbyDH7qtsR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mBZtzna5qQx"
      },
      "outputs": [],
      "source": [
        "#checking for missing value if there in dependent and independent variable\n",
        "dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UG_xodm546J"
      },
      "outputs": [],
      "source": [
        "#checking for duplicate values\n",
        "dataset.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLG9zWSK6PxO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#removing duplicated values\n",
        "dataset.drop_duplicates(keep='first',inplace=True)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to use 1 and 0 instead of spam and not spam respectively using label encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "dataset['target']=le.fit_transform(dataset['target'])   #dependent variable"
      ],
      "metadata": {
        "id": "t9-XjofCP8JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW5611O38Izd"
      },
      "source": [
        "# **Exploratory data analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY4PQM9R8MGJ"
      },
      "outputs": [],
      "source": [
        "dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PSp9N8-MP_z"
      },
      "outputs": [],
      "source": [
        "#plot of percentage of spam and ham messages in data\n",
        "import matplotlib.pyplot as plt\n",
        "plt.pie(dataset['target'].value_counts(),labels=['spam','ham'],autopct='%0.2f')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMhJ6hUuThRm"
      },
      "outputs": [],
      "source": [
        "# to get number of characters, words and sentences from text\n",
        "dataset['total_characters']=dataset['data'].apply(len)\n",
        "dataset['total_sentences']=dataset['data'].apply(lambda x:len(nltk.sent_tokenize(x)))\n",
        "dataset['total_words']=dataset['data'].apply(lambda x:len(nltk.word_tokenize(x)))\n",
        "\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUWbV3cvd0ZE"
      },
      "outputs": [],
      "source": [
        "#to describe 3 columns\n",
        "dataset[['total_characters','total_words','total_sentences']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dFFmkt6eXDv"
      },
      "outputs": [],
      "source": [
        "# describing about ham(not spam)\n",
        "dataset[dataset['target']==0][['total_characters','total_words','total_sentences']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_T0ZYxAgeneS"
      },
      "outputs": [],
      "source": [
        "# describing about spam( spam)\n",
        "dataset[dataset['target']==1][['total_characters','total_words','total_sentences']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKXwSSV-etBS"
      },
      "outputs": [],
      "source": [
        "#constructing histograms for comparing\n",
        "import seaborn as sns\n",
        "#comparing characters\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.histplot(dataset[dataset['target']==0]['total_characters'],color='blue')\n",
        "sns.histplot(dataset[dataset['target']==1]['total_characters'],color='red')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qruf6n84q7Xu"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(dataset,hue='target')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKad77pOrTAt"
      },
      "source": [
        "# **DATA PRE-PROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqhSlmEZrVbv"
      },
      "outputs": [],
      "source": [
        "#how to use vectorisation over here\n",
        "\n",
        "#lower case, tokenisation, removing special characters, removing stop words and punctuations , stemming\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "#defining function alag alag\n",
        "\n",
        "def transform_data(data):\n",
        "   #lower case\n",
        "    data=data.lower()\n",
        "    #words\n",
        "    data=nltk.word_tokenize(data)\n",
        "    y=[]\n",
        "    for i in data:\n",
        "       if i.isalnum():\n",
        "        y.append(i)\n",
        "    data=y[:]\n",
        "    y.clear()\n",
        "    #checking for stopwords and punctuations\n",
        "    for i in data:\n",
        "      if i not in stopwords.words('english') and i not in string.punctuation:\n",
        "        y.append(i)\n",
        "    #stemming\n",
        "    data=y[:]\n",
        "    y.clear()\n",
        "    ps=nltk.PorterStemmer()\n",
        "    for i in data:\n",
        "       y.append(ps.stem(i))\n",
        "    return \" \".join(y)\n",
        "\n",
        "dataset['transformed_data']=dataset['data'].apply(transform_data)\n",
        "\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ2GXe_7NOb7"
      },
      "outputs": [],
      "source": [
        "#word cloud of spam\n",
        "from wordcloud import WordCloud\n",
        "wc=WordCloud(width=500,height=500,min_font_size=10,background_color='white')\n",
        "spam_wc=wc.generate(dataset[dataset['target']==1]['transformed_data'].str.cat(sep=\" \"))\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.imshow(spam_wc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHm5jh5rOL_S"
      },
      "outputs": [],
      "source": [
        "#word cloud for ham\n",
        "ham_wc=wc.generate(dataset[dataset['target']==0]['transformed_data'].str.cat(sep=\" \"))\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.imshow(ham_wc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOQigM1aPKzy"
      },
      "outputs": [],
      "source": [
        "#storing in  list(text)\n",
        "spam_corpus=[]\n",
        "for msg in dataset[dataset['target']==1]['transformed_data'].tolist():\n",
        "  for word in msg.split():\n",
        "    spam_corpus.append(word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AH0rY4-QQX5"
      },
      "outputs": [],
      "source": [
        "len(spam_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZspvjPPUS8Iw"
      },
      "outputs": [],
      "source": [
        "#top used words spam\n",
        "from collections import Counter\n",
        "spam_corpus_dataset = pd.DataFrame(Counter(spam_corpus).most_common(30), columns=['Word', 'Frequency'])\n",
        "sns.barplot(x='Word', y='Frequency', data=spam_corpus_dataset)\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BKv09nbTAwp"
      },
      "outputs": [],
      "source": [
        "#top used words ham\n",
        "#storing in  list(text)\n",
        "ham_corpus=[]\n",
        "for msg in dataset[dataset['target']==0]['transformed_data'].tolist():\n",
        "  for word in msg.split():\n",
        "    ham_corpus.append(word)\n",
        "\n",
        "from collections import Counter\n",
        "ham_corpus_dataset = pd.DataFrame(Counter(ham_corpus).most_common(30), columns=['Word', 'Frequency'])\n",
        "sns.barplot(x='Word', y='Frequency', data=ham_corpus_dataset)\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ZUT0G--2hb"
      },
      "source": [
        "# **MODEL BUILD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Dlf096edc-i"
      },
      "outputs": [],
      "source": [
        "#to work on ml model we need numerical data[text to vector -> bagofwords, tfidf,wordstovec ]\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "cv=CountVectorizer()\n",
        "tfidf=TfidfVectorizer(max_features=3000)\n",
        "X=tfidf.fit_transform(dataset['transformed_data']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc3DvNF-_FjH"
      },
      "outputs": [],
      "source": [
        "y=dataset['target'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohz6abfUstFH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-5YaVd8tL5T"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEWB9RMEuUhS"
      },
      "outputs": [],
      "source": [
        "gnb=GaussianNB()\n",
        "mnb=MultinomialNB()\n",
        "bnb=BernoulliNB()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXtC08DuuaLZ"
      },
      "outputs": [],
      "source": [
        "gnb.fit(X_train,y_train)\n",
        "y_pred1=gnb.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred1))\n",
        "print(confusion_matrix(y_test,y_pred1))\n",
        "print(precision_score(y_test,y_pred1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEgCR-x0ugSa",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#giving max precision  when tfidf\n",
        "mnb.fit(X_train,y_train)\n",
        "y_pred2=mnb.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred2))\n",
        "print(confusion_matrix(y_test,y_pred2))\n",
        "print(precision_score(y_test,y_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78YY7AyGuqTC"
      },
      "outputs": [],
      "source": [
        "bnb.fit(X_train,y_train)\n",
        "y_pred3=bnb.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred3))\n",
        "print(confusion_matrix(y_test,y_pred3))\n",
        "print(precision_score(y_test,y_pred3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUpb5t1DbxdJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFtU4UbOcugh"
      },
      "outputs": [],
      "source": [
        "svc=SVC(kernel='sigmoid',gamma=1.0)\n",
        "knc=KNeighborsClassifier()\n",
        "mnb=MultinomialNB()\n",
        "dtc=DecisionTreeClassifier(max_depth=5)\n",
        "lrc=LogisticRegression(solver='liblinear',penalty='l1')\n",
        "rfc=RandomForestClassifier(n_estimators=50,random_state=2)\n",
        "abc=AdaBoostClassifier(n_estimators=50,random_state=2)\n",
        "bc=BaggingClassifier(n_estimators=50,random_state=2)\n",
        "etc=ExtraTreesClassifier(n_estimators=50,random_state=2)\n",
        "gbdt=GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
        "xgb=XGBClassifier(n_estimators=50,random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFe5Iie-dF8C"
      },
      "outputs": [],
      "source": [
        "clfs={\n",
        "    'SVC':svc,\n",
        "    'KNN':knc,\n",
        "    'MNB':mnb,\n",
        "    'DT': dtc,\n",
        "    'LR':lrc,\n",
        "    'RFC':rfc,\n",
        "    'ABC':abc,\n",
        "    'BC':bc,\n",
        "    'ETC':etc,\n",
        "    'GBDT':gbdt,\n",
        "    'XGB':xgb\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpxuRcSPifu2"
      },
      "outputs": [],
      "source": [
        "def train_classifier(clf, X_train, y_train, X_test, y_test):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy=accuracy_score(y_test,y_pred)\n",
        "    precision=precision_score(y_test,y_pred)\n",
        "    return accuracy,precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afL5Ujj9iosk"
      },
      "outputs": [],
      "source": [
        "accuracy_scores=[]\n",
        "precision_scores=[]\n",
        "\n",
        "for name,clf in clfs.items():\n",
        "  current_accuracy,current_precision=train_classifier(clf,X_train,y_train,X_test,y_test)\n",
        "  print(\"For \",name)\n",
        "  print(\"Accuracy - \",current_accuracy)\n",
        "  print(\"Precision - \",current_precision)\n",
        "  accuracy_scores.append(current_accuracy)\n",
        "  precision_scores.append(current_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGVOuCqujaP8"
      },
      "outputs": [],
      "source": [
        "performance_dataset=pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)\n",
        "performance_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-U8bh61jqfK"
      },
      "source": [
        "# **Model Improvement **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfmukeUije_r"
      },
      "outputs": [],
      "source": [
        "# changing vectorisation method\n",
        "# changing their max features\n",
        "# appending any column like character , alphabet\n",
        "#     X=np.hstack((X, df['num_characters'].values.reshape [-1,1]))\n",
        "#     y=df['target].values\n",
        "# using voting classifier\n",
        "# using stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo97o3tJkkIL"
      },
      "outputs": [],
      "source": [
        "# #voting classifier\n",
        "# from sklearn.ensemble import VotingClassifier\n",
        "# svc=SVC(kernel='sigmoid',gamma=1.0,probability=True)\n",
        "# mnb=MultinomialNB()\n",
        "# etc=ExtraTreesClassifier(n_estimators=50,random_state=2)\n",
        "# voting=VotingClassifier(estimators=[('svm',svc),('nb',mnb),('et',etc)],voting='soft')\n",
        "# voting.fit(X_train,y_train)\n",
        "# y_pred=voting.predict(X_test)\n",
        "# print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
        "# print(\"Precision\",precision_score(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3fny--BcFER"
      },
      "outputs": [],
      "source": [
        "# #appling stacking\n",
        "# estimators=[('svm',svc),('nb',mnb),('et',etc)]\n",
        "# final_estimator=RandomForestClassifier()\n",
        "# from sklearn.ensemble import StackingClassifier\n",
        "# clf=StackingClassifier(estimators=estimators,final_estimator=final_estimator)\n",
        "# clf.fit(X_train,y_train)\n",
        "# y_pred=clf.predict(X_test)\n",
        "# print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
        "# print(\"Precision\",precision_score(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgQyzsk9VCPG"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump(tfidf,open('vectorizer.pkl','wb'))\n",
        "pickle.dump(mnb,open('model.pkl','wb'))\n",
        "with open('trained_model.pkl', 'wb') as f:\n",
        "    pickle.dump(mnb, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXJ1BymohtTWBbFthSXyoN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}